#!/usr/bin/env python3
"""
A simple script to parse the CSV output from the loadtime reporting tool (see
https://github.com/tendermint/tendermint/tree/main/test/loadtime/cmd/report).

Produces a plot of average transaction latency vs total transaction throughput
according to the number of load testing tool WebSocket connections to the
Tendermint node.
"""

import argparse
import csv
import logging
import sys
import matplotlib.pyplot as plt
import numpy as np

DEFAULT_TITLE = "Tendermint latency vs throughput"
DEFAULT_DURATION = 90.0


def main():
    parser = argparse.ArgumentParser(
        description="Renders a latency vs throughput diagram "
        "for a set of transactions provided by the loadtime reporting tool",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-d',
                        '--duration',
                        type=float,
                        default=DEFAULT_DURATION,
                        help='Duration of experiment, in seconds')
    parser.add_argument('-t',
                        '--title',
                        default=DEFAULT_TITLE,
                        help='Plot title')
    parser.add_argument('output_image',
                        help='Output image file (in PNG format)')
    parser.add_argument(
        'input_csv_file',
        nargs='+',
        help="CSV input file from which to read transaction data "
        "- must have been generated by the loadtime reporting tool")
    args = parser.parse_args()

    logging.basicConfig(format='%(levelname)s\t%(message)s',
                        stream=sys.stdout,
                        level=logging.INFO)
    plot_latency_vs_throughput(args.input_csv_file,
                               args.output_image,
                               duration=args.duration,
                               title=args.title)


def plot_latency_vs_throughput(input_files,
                               output_image,
                               duration=DEFAULT_DURATION,
                               title=DEFAULT_TITLE):
    avg_latencies, throughput_rates = parse_input_files(input_files,
                                                        duration=duration)

    fig, ax = plt.subplots()

    connections = sorted(avg_latencies.keys())
    for c in connections:
        tr = np.array(throughput_rates[c])
        al = np.array(avg_latencies[c])
        label = '%d connection%s' % (c, '' if c == 1 else 's')
        ax.plot(tr, al, 'o-', label=label)

    ax.set_title(title)
    ax.set_xlabel('Throughput rate (tx/s)')
    ax.set_ylabel('Average latency (s)')

    plt.legend(loc='upper left')
    plt.savefig(output_image)


def parse_input_files(input_files, duration=DEFAULT_DURATION):
    total_txs = 0
    # Per-connection latency vs throughput rates
    latency_throughput = {}

    for input_file in input_files:
        logging.info('Reading %s...' % input_file)
        latencies_sum = 0.0
        connections = None
        experiment_id = None
        tx_count = 0

        with open(input_file, 'rt') as inf:
            reader = csv.reader(inf)
            for row in reader:
                # We don't always have the header row
                if row[0] == 'experiment_id':
                    continue

                tx = {
                    'experiment_id': row[0],
                    'block_time': int(row[1]),
                    'duration_ns': int(row[2]),
                    'tx_hash': row[3],
                    'connections': int(row[4]),
                    'rate': int(row[5]),
                    'size': int(row[6]),
                }
                if experiment_id is None:
                    experiment_id = tx['experiment_id']
                elif experiment_id != tx['experiment_id']:
                    logging.error(
                        'Cannot have multiple experiment IDs in the same input'
                        ' file. Found %s and %s in %s' %
                        (experiment_id, tx['experiment_id'], input_file))
                    sys.exit(1)

                if connections is None:
                    connections = tx['connections']
                elif connections != tx['connections']:
                    logging.error(
                        'Cannot have multiple numbers of connections in the '
                        'same input file. Found %s and %s in %s' %
                        (connections, tx['connections'], input_file))
                    sys.exit(1)

                # Latency durations are in nanoseconds - convert to seconds
                latencies_sum += tx['duration_ns'] / (10**9)
                tx_count += 1

        if connections not in latency_throughput:
            latency_throughput[connections] = []

        avg_latency = latencies_sum / tx_count
        throughput_rate = tx_count / duration
        latency_throughput[connections].append({
            'avg_latency':
            avg_latency,
            'throughput_rate':
            throughput_rate,
        })
        total_txs += tx_count

    # Sort data points for each connection in order of ascending throughput
    # rate and extract the data series separately
    connections = sorted(latency_throughput.keys())
    avg_latencies = {}
    throughput_rates = {}
    for c in connections:
        latency_throughput[c] = sorted(latency_throughput[c],
                                       key=lambda lt: lt['throughput_rate'])
        avg_latencies[c] = []
        throughput_rates[c] = []
        for lt in latency_throughput[c]:
            avg_latencies[c].append(lt['avg_latency'])
            throughput_rates[c].append(lt['throughput_rate'])
            logging.info('Connections = %d\tThroughput rate = %.6f tx/sec'
                         '\tAvg latency = %.6fs' %
                         (c, lt['throughput_rate'], lt['avg_latency']))

    logging.info('Read %d transactions from %d file(s)' %
                 (total_txs, len(input_files)))

    return (avg_latencies, throughput_rates)


if __name__ == "__main__":
    main()
